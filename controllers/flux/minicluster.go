/*
Copyright 2022-2023 Lawrence Livermore National Security, LLC
 (c.f. AUTHORS, NOTICE.LLNS, COPYING)

This is part of the Flux resource manager framework.
For details, see https://github.com/flux-framework.

SPDX-License-Identifier: Apache-2.0
*/

package controllers

import (
	"context"
	"fmt"
	"strings"

	jobctrl "github.com/flux-framework/flux-operator/pkg/job"

	batchv1 "k8s.io/api/batch/v1"
	corev1 "k8s.io/api/core/v1"

	ctrl "sigs.k8s.io/controller-runtime"

	"k8s.io/apimachinery/pkg/api/errors"
	"k8s.io/apimachinery/pkg/types"

	api "github.com/flux-framework/flux-operator/api/v1alpha1"
)

var (
	HostfileName   = "hostfile"
	CurveCertKey   = "curve.cert"
	mungeMountName = "munge-key"
)

// This is a MiniCluster! A MiniCluster is associated with a running MiniCluster and include:
// 1. An indexed job with some number of pods
// 2. Config maps for secrets and other things.
// 3. We "launch" a job by starting the Indexed job on the connected nodes
// ensureMiniCluster creates a new MiniCluster, a stateful set for running flux!
func (r *MiniClusterReconciler) ensureMiniCluster(
	ctx context.Context,
	cluster *api.MiniCluster,
) (ctrl.Result, error) {

	// Ensure the configs are created (for volume sources)
	_, result, err := r.getConfigMap(ctx, cluster, "flux-config", cluster.FluxConfigMapName())
	if err != nil {
		return result, err
	}

	// Add initial config map with entrypoint scripts (wait.sh, start.sh, etc.)
	_, result, err = r.getConfigMap(ctx, cluster, "entrypoint", cluster.EntrypointConfigMapName())
	if err != nil {
		return result, err
	}

	// Generate the curve certificate config map, unless already exists
	if cluster.Spec.Flux.CurveCertSecret == "" {
		_, result, err = r.getConfigMap(ctx, cluster, "cert", cluster.CurveConfigMapName())
		if err != nil {
			return result, err
		}
	}

	// Prepare volumes, if requested, to be available to containers
	for volumeName, volume := range cluster.Spec.Volumes {
		_, result, err = r.getPersistentVolume(ctx, cluster, volumeName, volume)
		if err != nil {
			return result, err
		}
		_, result, err = r.getPersistentVolumeClaim(ctx, cluster, volumeName, volume)
		if err != nil {
			return result, err
		}
	}

	// Any extra service containers (running alongside the cluster)
	// For now run these in the same pod, one service pod
	if len(cluster.Spec.Services) > 0 {
		_, result, err = r.ensureServicePod(ctx, cluster)
		if err != nil {
			return result, err
		}
	}

	// Create headless service for the MiniCluster OR single service for the broker
	selector := map[string]string{"job-name": cluster.Name}

	// If we are adding a minimal service to the index 0 pod only
	if cluster.Spec.Flux.MinimalService {
		selector = map[string]string{"job-index": "0"}
	}

	result, err = r.exposeServices(ctx, cluster, cluster.Spec.Network.HeadlessName, selector)
	if err != nil {
		return result, err
	}

	// Create the batch job that brings it all together!
	// A batchv1.Job can hold a spec for containers that use the configs we just made
	mc, result, err := r.getMiniCluster(ctx, cluster)
	if err != nil {
		return result, err
	}

	// If the sizes are different, we patch to update.
	// This would be an explicit update from a user or application via the CRD to scale up/down
	// The Flux Operator can't tell the difference between these two, but honors maxSize
	// (the size the Flux broker leader knows about) and updates both Spec.Size and Status.Size
	if *mc.Spec.Parallelism != cluster.Spec.Size {
		r.log.Info("MiniCluster", "Size", mc.Spec.Parallelism, "Requested Size", cluster.Spec.Size)
		result, err := r.resizeCluster(ctx, mc, cluster)
		if err != nil {
			return result, err
		}
	}

	// Add selector to allow horizontal pod autoscaler
	// This would be done via a request to a running metrics server
	// If there is no autoscaler, has no impact. The .Status.Size
	// should already be updated via the function above.
	result, err = r.addScaleSelector(ctx, cluster)
	if err != nil {
		return result, err
	}

	// Expose other sidecar container services
	for _, container := range cluster.Spec.Containers {

		// Assume now services only available TO flux runner
		if container.RunFlux || len(container.Ports) == 0 {
			continue
		}

		// Service name corresponds to container, but selector is pod-specific
		selector := map[string]string{"app.kubernetes.io/name": cluster.Name}
		result, err = r.exposeService(ctx, cluster, container.Name, selector, container.Ports)
		if err != nil {
			return result, err
		}
	}

	// Add the single label for the broker pod
	result, err = r.addBrokerLabel(ctx, cluster)
	if err != nil {
		return result, err
	}

	// If we get here, update the status to be ready
	status := jobctrl.GetCondition(cluster)
	if status != jobctrl.ConditionJobReady {
		clusterCopy := cluster.DeepCopy()
		jobctrl.FlagConditionReady(clusterCopy)
		r.Status().Update(ctx, clusterCopy)
	}

	// And we re-queue so the Ready condition triggers next steps!
	return ctrl.Result{Requeue: true}, nil
}

// cleanupPodsStorage looks for the existing job, and cleans up if completed
func (r *MiniClusterReconciler) cleanupPodsStorage(
	ctx context.Context,
	cluster *api.MiniCluster,
) (ctrl.Result, error) {

	// Find the broker pod and determine if finished
	completed := false
	for _, pod := range r.getMiniClusterPods(ctx, cluster).Items {
		if !strings.HasPrefix(pod.Name, fmt.Sprintf("%s-0", cluster.Name)) {
			continue
		}

		// If it's succeeded or failed, we call that finished
		// https://pkg.go.dev/k8s.io/api@v0.25.0/core/v1#PodPhase
		if pod.Status.Phase == corev1.PodSucceeded || pod.Status.Phase == corev1.PodFailed {
			completed = true
			break
		}
	}

	// Cut out early if not completed
	if !completed {
		r.log.Info("MiniCluster", "Job Status", "Not Completed")
		return ctrl.Result{Requeue: true}, nil
	}
	r.log.Info("MiniCluster", "Job Status", "Completed")

	// Delete the MiniCluster first
	// If we don't, it will keep re-creating the assets and loop forever :)
	r.Client.Delete(ctx, cluster)

	// The job deletion should handle pods, next delete pvc and pv per each volume
	// Must be deleted in that order, per internet advice :)
	for volumeName := range cluster.Spec.Volumes {
		volumeSpec := cluster.Spec.Volumes[volumeName]

		claimName := fmt.Sprintf("%s-claim", volumeName)

		// Only delete if we retrieve without error and user has requested
		claim, err := r.getExistingPersistentVolumeClaim(ctx, cluster, claimName)
		if err != nil {
			r.log.Info("Volume Claim", "Deletion", claim.Name)
			r.Client.Delete(ctx, claim)
		}

		// Different request to delete
		if volumeSpec.Delete {
			pv, err := r.getExistingPersistentVolume(ctx, cluster, volumeName)
			if err != nil {
				r.log.Info("Volume", "Deletion", pv.Name)
				r.Client.Delete(ctx, pv)
			}
		}
	}
	return ctrl.Result{Requeue: false}, nil
}

// getExistingJob gets an existing job that matches the MiniCluster CRD
func (r *MiniClusterReconciler) getExistingJob(
	ctx context.Context,
	cluster *api.MiniCluster,
) (*batchv1.Job, error) {

	existing := &batchv1.Job{}
	err := r.Get(
		ctx,
		types.NamespacedName{
			Name:      cluster.Name,
			Namespace: cluster.Namespace,
		},
		existing,
	)
	return existing, err
}

// getMiniCluster does an actual check if we have a batch job in the namespace
func (r *MiniClusterReconciler) getMiniCluster(
	ctx context.Context,
	cluster *api.MiniCluster,
) (*batchv1.Job, ctrl.Result, error) {

	// Look for an existing job
	existing, err := r.getExistingJob(ctx, cluster)

	// Create a new job if it does not exist
	if err != nil {

		if errors.IsNotFound(err) {
			job, err := NewMiniClusterJob(cluster)
			ctrl.SetControllerReference(cluster, job, r.Scheme)

			if err != nil {
				r.log.Error(
					err, "Failed to create new MiniCluster Batch Job",
					"Namespace:", job.Namespace,
					"Name:", job.Name,
				)
				return job, ctrl.Result{}, err
			}

			r.log.Info(
				"âœ¨ Creating a new MiniCluster Batch Job âœ¨",
				"Namespace:", job.Namespace,
				"Name:", job.Name,
			)

			err = r.New(ctx, job)
			if err != nil {
				r.log.Error(
					err,
					"Failed to create new MiniCluster Batch Job",
					"Namespace:", job.Namespace,
					"Name:", job.Name,
				)
				return job, ctrl.Result{}, err
			}
			// Successful - return and requeue
			return job, ctrl.Result{Requeue: true}, nil

		} else if err != nil {
			r.log.Error(err, "Failed to get MiniCluster Batch Job")
			return existing, ctrl.Result{}, err
		}

	} else {
		r.log.Info(
			"ðŸŽ‰ Found existing MiniCluster Batch Job ðŸŽ‰",
			"Namespace:", existing.Namespace,
			"Name:", existing.Name,
		)
	}
	return existing, ctrl.Result{}, err
}
